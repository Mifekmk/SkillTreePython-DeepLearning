{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch01_04_DNN기초-이진분류모델.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mifekmk/SkillTreePython-DeepLearning/blob/main/01.%EB%94%A5%EB%9F%AC%EB%8B%9Dwith%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0-basic/ch01_04_DNN%EA%B8%B0%EC%B4%88_%EC%9D%B4%EC%A7%84%EB%B6%84%EB%A5%98%EB%AA%A8%EB%8D%B8_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch01.04 DNN기초 - 이진분류 모델\n",
        "\n",
        "\n",
        "---\n",
        "* 날짜: 2022-06-28\n",
        "* 이름: 김민규\n",
        "\n",
        "## 학습내용\n",
        "    - 딥러닝을 이용한 이진분류 모델 구현\n",
        "    - 적절한 손실함수와 최적화 함수 정의\n",
        "    - 평가 및 예측\n",
        "\n",
        "## 학습자료\n",
        "\n",
        "* 모두의딥러닝 11장, 13장\n",
        "* 데이터\n",
        "  * `sornar.csv`\n",
        "  * `pima-indians-diabetes.csv`\n",
        "\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "seed=1\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "```\n",
        "\n",
        "```\n",
        "https://github.com/yebiny/SkillTreePython-DeepLearning.git\n",
        "```"
      ],
      "metadata": {
        "id": "sWs2kEC1_b-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "seed=1\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "3dvXKSSnDQ9V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/yebiny/SkillTreePython-DeepLearning.git"
      ],
      "metadata": {
        "id": "VIgPnWNbUt_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca86334-41b5-40a4-834c-e5d850b04192"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SkillTreePython-DeepLearning' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 소나 데이터 광물 예측\n",
        "---\n",
        "\n",
        "> 1988년 존스홉킨스 대학교의 세즈노프스키(Sejnowski) 교수는 광석과 일반 돌을 가져다 놓고 음파 탐지기를 쏜 후 그 결과를 데이터를 정리했습니다. 신경망이 광석과 돌을 얼마나 잘 구분하는지 알아보도록 합시다.\n",
        "\n",
        "```\n",
        "- 0~59 : 음파 탐지기를 이용해 얻은 값\n",
        "- 60: 광석 구분 {R, M}\n",
        "```\n",
        "<p align='center'>\n",
        "<img src=https://github.com/yebiny/SkillTreePython-DeepLearning/blob/main/imgs/ch0104-01.PNG?raw=true width=450>\n",
        "</p>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KOmt3w8_rVe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | 데이터 준비"
      ],
      "metadata": {
        "id": "TmgkVAdIfngb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **데이터 로드**\n",
        "* `sonar.csv`"
      ],
      "metadata": {
        "id": "cHXodXKif2BB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2sh64rMrANLD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "7879537e-f54d-4df0-b750-6824195855dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       0       1       2       3       4       5       6       7       8   \\\n",
              "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
              "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
              "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
              "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
              "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
              "\n",
              "       9   ...      51      52      53      54      55      56      57  \\\n",
              "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
              "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
              "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
              "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
              "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
              "\n",
              "       58      59  60  \n",
              "0  0.0090  0.0032   R  \n",
              "1  0.0052  0.0044   R  \n",
              "2  0.0095  0.0078   R  \n",
              "3  0.0040  0.0117   R  \n",
              "4  0.0107  0.0094   R  \n",
              "\n",
              "[5 rows x 61 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3ff2c0c-635b-4248-afa8-c3ec50fd545c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 61 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3ff2c0c-635b-4248-afa8-c3ec50fd545c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c3ff2c0c-635b-4248-afa8-c3ec50fd545c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c3ff2c0c-635b-4248-afa8-c3ec50fd545c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data_path ='/content/SkillTreePython-DeepLearning/dataset/sonar.csv'\n",
        "df = pd.read_csv(data_path,\n",
        "                 header=None)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **데이터 전처리**"
      ],
      "metadata": {
        "id": "0NL7LyHXrhoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def path2data_sonar(path, seed=1):\n",
        "    # 데이터 적절히 불러오기\n",
        "    df = pd.read_csv(data_path,\n",
        "                     header=None, # 첫번째 행이 데이터 (칼럼이 없음)\n",
        "                     )\n",
        "    # x-y 분할\n",
        "    x = df.values[:,0:-1] # 모든 행(샘플), 0부터 59까지 열(속성)\n",
        "    y = df.values[:,-1] # 모든 행(샘플), 60번째 열(속성)\n",
        "    \n",
        "    # 정규화 (x의 모든 속성, y는 하지 않음)\n",
        "    scaler = StandardScaler()\n",
        "    x = scaler.fit_transform(x)\n",
        "\n",
        "    # 라벨링 (y 라벨링, x는 하지 않음)\n",
        "    labeling = LabelEncoder()\n",
        "    y = labeling.fit_transform(y)\n",
        "\n",
        "    # train-test 데이터 분할\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3,\n",
        "                                                        random_state=seed, stratify=y) # stratify는 train_test 분할 시 클레스 비율을 비슷하게 유지해 줍니다.\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "data_path = '/content/SkillTreePython-DeepLearning/dataset/sonar.csv'\n",
        "x_train, x_test, y_train, y_test = path2data_sonar(data_path)\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "print(x_train[1], y_train[1])"
      ],
      "metadata": {
        "id": "CoWLi5NgraBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45bb24ba-e111-4b26-91a7-a0491a89b590"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(145, 60) (63, 60) (145,) (63,)\n",
            "[-0.73091423  0.14791278  0.43217734 -0.193733   -0.27973353 -1.17141213\n",
            " -1.11368502 -1.1830759  -0.61643936  0.02789771 -0.54396541 -0.43025161\n",
            "  0.20832115 -0.57696915 -1.27359777 -0.60100177 -0.1911599  -0.2737382\n",
            "  0.34226731  0.25514539 -0.11259861 -0.65372658 -0.80968198 -1.27254722\n",
            " -0.12615184  0.8203173   0.79833763 -0.82844031 -1.28832161 -1.57084217\n",
            " -1.63565539 -0.34053744  0.08145143 -0.41196074 -0.34881305  0.09887612\n",
            "  0.77837874  0.34473565  1.128427    1.06935769  0.14029395  0.01132786\n",
            " -0.08251782  0.11825963 -0.54032991 -0.66033662 -0.69576299 -0.9575622\n",
            " -0.19317929 -0.27318109 -0.69023617 -0.53275402 -0.71118685  0.88684983\n",
            "  1.34495706 -0.49309408  0.62024838  1.80515572  1.09602656 -0.0612105 ] 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 예측값 분포 확인"
      ],
      "metadata": {
        "id": "J49Ys-SNBKyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(121);sns.countplot(x=y_train);plt.title('trainset')\n",
        "plt.subplot(122);sns.countplot(x=y_test);plt.title('testset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3vlsRwuoBMcY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "59a95235-835b-4fe1-925f-2dbe5e2f9e50"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | 모델\n",
        "\n",
        "\n",
        "<p align='center'>\n",
        "<img src=https://github.com/yebiny/SkillTreePython-DeepLearning/blob/main/imgs/ch0104-02.PNG?raw=true width=500>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "DHshtWB9r0be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **모델 생성**"
      ],
      "metadata": {
        "id": "J94jFpWyr2k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers, utils\n",
        "\n",
        "def build_model():\n",
        "    x = layers.Input(shape=(60,)) # 데이터가 주어지면 인풋의 shape는 고정\n",
        "    z = layers.Dense(30,activation='relu')(x)\n",
        "    y = layers.Dense(1, activation='sigmoid')(z) # 데이터가 주어지면 아웃풋의 shape는 고정 \n",
        "    # (이진분류문제는 마지막 아웃풋 shape가 1, 마지막 활성함수는 sigmoid를 사용합니다.)\n",
        "    model = models.Model(x,y, name='sonar_classifier')\n",
        "    return model"
      ],
      "metadata": {
        "id": "5BZG1z90rsGc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **모델 시각화**\n",
        "\n",
        "* `model.summary()`\n",
        "* `utils.plot_model()`"
      ],
      "metadata": {
        "id": "1yqXLbEyx5QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "# 시각화\n",
        "utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "dKCT38Fsr6dF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "021366ac-f804-4d97-a5c3-b2a9f2a4b0c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sonar_classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 60)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 30)                1830      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 31        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,861\n",
            "Trainable params: 1,861\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | 학습"
      ],
      "metadata": {
        "id": "gLuvsUFmwzPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **모델 컴파일**\n",
        "\n",
        "```\n",
        "model.compile(loss, optimizer, metrics) \n",
        "```\n",
        "* `loss` : 'binary_crossentropy'\n",
        "* `optimizer` : 'adam'\n",
        "* `metrics`: 'accuracy'\n"
      ],
      "metadata": {
        "id": "fwQboNHp3EHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = 'accuracy')"
      ],
      "metadata": {
        "id": "SASPIskxgfRY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **학습**\n",
        "\n",
        "```\n",
        "model.fit(x_train, y_train, epochs, batch_size)\n",
        "```\n",
        "* `x_train` : 넘파이 혹은 텐서 형식의 인풋 데이터\n",
        "* `y_train` : 넘파이 혹은 텐서 형식의 아웃풋 데이터\n",
        "* `epochs` : 학습 횟수\n",
        "* `batch_size` : 배치 사이즈 (업데이트 한번에 사용될 샘플 개수)"
      ],
      "metadata": {
        "id": "JukUpWB4gfRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          epochs = 200,\n",
        "          batch_size = 16)"
      ],
      "metadata": {
        "id": "9bgbpUhogfRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6fc074-bfc2-495f-b613-317c8f0f5e1a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "10/10 [==============================] - 1s 3ms/step - loss: 0.8359 - accuracy: 0.4414\n",
            "Epoch 2/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.7100 - accuracy: 0.5172\n",
            "Epoch 3/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.6332 - accuracy: 0.5862\n",
            "Epoch 4/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.6897\n",
            "Epoch 5/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.5245 - accuracy: 0.7172\n",
            "Epoch 6/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7241\n",
            "Epoch 7/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4627 - accuracy: 0.7517\n",
            "Epoch 8/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.4374 - accuracy: 0.7655\n",
            "Epoch 9/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.7931\n",
            "Epoch 10/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8000\n",
            "Epoch 11/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8207\n",
            "Epoch 12/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3618 - accuracy: 0.8414\n",
            "Epoch 13/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8552\n",
            "Epoch 14/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.3335 - accuracy: 0.8828\n",
            "Epoch 15/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3204 - accuracy: 0.8759\n",
            "Epoch 16/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8828\n",
            "Epoch 17/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2957 - accuracy: 0.8966\n",
            "Epoch 18/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.2857 - accuracy: 0.9103\n",
            "Epoch 19/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2751 - accuracy: 0.9241\n",
            "Epoch 20/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2647 - accuracy: 0.9310\n",
            "Epoch 21/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2554 - accuracy: 0.9310\n",
            "Epoch 22/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2456 - accuracy: 0.9379\n",
            "Epoch 23/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2368 - accuracy: 0.9379\n",
            "Epoch 24/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.9448\n",
            "Epoch 25/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9448\n",
            "Epoch 26/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 0.9448\n",
            "Epoch 27/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.2063 - accuracy: 0.9517\n",
            "Epoch 28/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.9586\n",
            "Epoch 29/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1926 - accuracy: 0.9586\n",
            "Epoch 30/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1864 - accuracy: 0.9586\n",
            "Epoch 31/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1803 - accuracy: 0.9724\n",
            "Epoch 32/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1746 - accuracy: 0.9724\n",
            "Epoch 33/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1694 - accuracy: 0.9793\n",
            "Epoch 34/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1635 - accuracy: 0.9793\n",
            "Epoch 35/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1578 - accuracy: 0.9862\n",
            "Epoch 36/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1532 - accuracy: 0.9862\n",
            "Epoch 37/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1490 - accuracy: 0.9862\n",
            "Epoch 38/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.9862\n",
            "Epoch 39/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1411 - accuracy: 0.9931\n",
            "Epoch 40/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.1361 - accuracy: 0.9931\n",
            "Epoch 41/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1318 - accuracy: 0.9931\n",
            "Epoch 42/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9931\n",
            "Epoch 43/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1233 - accuracy: 0.9931\n",
            "Epoch 44/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1196 - accuracy: 0.9931\n",
            "Epoch 45/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1159 - accuracy: 0.9931\n",
            "Epoch 46/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.9931\n",
            "Epoch 47/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1098 - accuracy: 0.9931\n",
            "Epoch 48/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1062 - accuracy: 0.9931\n",
            "Epoch 49/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1030 - accuracy: 0.9931\n",
            "Epoch 50/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.1001 - accuracy: 0.9931\n",
            "Epoch 51/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0973 - accuracy: 0.9931\n",
            "Epoch 52/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9931\n",
            "Epoch 53/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0921 - accuracy: 0.9931\n",
            "Epoch 54/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0892 - accuracy: 0.9931\n",
            "Epoch 55/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0865 - accuracy: 0.9931\n",
            "Epoch 56/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0840 - accuracy: 0.9931\n",
            "Epoch 57/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.9931\n",
            "Epoch 58/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0792 - accuracy: 0.9931\n",
            "Epoch 59/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0769 - accuracy: 0.9931\n",
            "Epoch 60/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9931\n",
            "Epoch 61/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0707 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0689 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0687 - accuracy: 0.9931\n",
            "Epoch 65/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9931\n",
            "Epoch 66/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0674 - accuracy: 0.9931\n",
            "Epoch 67/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9931\n",
            "Epoch 68/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0608 - accuracy: 0.9931\n",
            "Epoch 69/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0582 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0565 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0551 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0446 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0414 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0402 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0394 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0358 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0343 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0332 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0316 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0281 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0164 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0122 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0084 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0059 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f372cf3dc50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | 검증 및 예측\n"
      ],
      "metadata": {
        "id": "DyFOnqyNw5_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **검증**\n",
        "\n",
        "```\n",
        "model.evaluate(x_test, y_test)\n",
        "```\n",
        "* `x_test` : 넘파이 혹은 텐서 형식의 인풋 테스트 데이터\n",
        "* `y_test` : 넘파이 혹은 텐서 형식의 아웃풋 테스트 데이터"
      ],
      "metadata": {
        "id": "qlalKZ-a04c3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "uHGyMlf60zng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601b1ea6-58f0-456c-95ab-bb2fef6fd50c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4233 - accuracy: 0.9048\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4233468472957611, 0.9047619104385376]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **예측**\n",
        "\n",
        "```\n",
        "model.predict(x_test)\n",
        "```\n",
        "* `x_test` : 넘파이 혹은 텐서 형식의 인풋 테스트 데이터\n",
        "* `y_test` : 넘파이 혹은 텐서 형식의 아웃풋 테스트 데이터"
      ],
      "metadata": {
        "id": "BITPUIOj07Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "print(y_pred[0], y_test[0])\n",
        "y_pred = y_pred.flatten()\n",
        "y_test.shape,y_pred.shape "
      ],
      "metadata": {
        "id": "KfDHL4PRv8YD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84dd8db7-51d3-42da-8a83-7245bbfbbb56"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8380451] 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((63,), (63,))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.round(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqqXBEFWI3OS",
        "outputId": "ab381b92-5349-49d9-ffe5-97a76ca44be6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
              "       1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
              "       0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
              "       1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 예측 시각화"
      ],
      "metadata": {
        "id": "gThN4RCR0_HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# 함수 인풋 배열은 정수가 되야 된다.\n",
        "cm = confusion_matrix(y_test, np.round(y_pred))\n",
        "print(cm)\n",
        "ConfusionMatrixDisplay(cm).plot(cmap='Blues')"
      ],
      "metadata": {
        "id": "TG533VyLwDaK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "7efce7c2-b8e4-4d66-cbb2-60647b31d85f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[30  4]\n",
            " [ 2 27]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f3728706ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 과제\n",
        "---"
      ],
      "metadata": {
        "id": "ZFrRTgcyipYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | 과제1. 이진분류 문제 정리\n",
        "\n",
        "* 예측값의 범위 : 0 에서 1 사이의 확률 값\n",
        "* 예측값의 shape : (샘플 개수, ) 혹은 (샘플 개수, 1)\n",
        "* 아웃풋 레이어의 노드 개수 : 1\n",
        "* 아웃풋 레이어의 activation : sigmoid\n",
        "* 손실함수 (loss) : binary crossentropy\n",
        "* 평가함수 (metrics) : accuracy"
      ],
      "metadata": {
        "id": "xjnJSLgXK_Pp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | 과제2. 한 셀에 코드 정리하기\n",
        "\n",
        "* 추가 연습\n",
        "  * 모델의 깊이(depth)를 늘려가며 학습해 보세요.\n",
        "  * 모델의 너비(width)를 늘려가며 학습해 보세요."
      ],
      "metadata": {
        "id": "ZvFAB78P1B_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def path2data_sonar(path, seed=1):\n",
        "    # 데이터 적절히 불러오기\n",
        "    df = pd.read_csv(data_path,\n",
        "                     header=None, # 첫번째 행이 데이터 (칼럼이 없음)\n",
        "                     )\n",
        "    # x-y 분할\n",
        "    x = df.values[:,0:-1] # 모든 행(샘플), 0부터 59까지 열(속성)\n",
        "    y = df.values[:,-1] # 모든 행(샘플), 60번째 열(속성)\n",
        "    \n",
        "    # 정규화 (x의 모든 속성, y는 하지 않음)\n",
        "    scaler = StandardScaler()\n",
        "    x = scaler.fit_transform(x)\n",
        "\n",
        "    # 라벨링 (y 라벨링, x는 하지 않음)\n",
        "    labeling = LabelEncoder()\n",
        "    y = labeling.fit_transform(y)\n",
        "\n",
        "    # train-test 데이터 분할\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3,\n",
        "                                                        random_state=seed, stratify=y) # stratify는 train_test 분할 시 클레스 비율을 비슷하게 유지해 줍니다.\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "data_path = '/content/SkillTreePython-DeepLearning/dataset/sonar.csv'\n",
        "x_train, x_test, y_train, y_test = path2data_sonar(data_path)\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "print(x_train[1], y_train[1])\n",
        "\n",
        "from tensorflow.keras import models, layers, utils\n",
        "\n",
        "def build_model():\n",
        "    x = layers.Input(shape=(60,)) # 데이터가 주어지면 인풋의 shape는 고정\n",
        "    z = layers.Dense(50,activation='relu')(x)\n",
        "    z = layers.Dense(40,activation='relu')(x)  \n",
        "    z = layers.Dense(30,activation='relu')(x)\n",
        "    z = layers.Dense(25,activation='relu')(x)\n",
        "    z = layers.Dense(17,activation='relu')(x)\n",
        "    z = layers.Dense(10,activation='relu')(x)\n",
        "    z = layers.Dense(6,activation='relu')(x)    \n",
        "    y = layers.Dense(1, activation='sigmoid')(z) # 데이터가 주어지면 아웃풋의 shape는 고정 \n",
        "    # (이진분류문제는 마지막 아웃풋 shape가 1, 마지막 활성함수는 sigmoid를 사용합니다.)\n",
        "    model = models.Model(x,y, name='sonar_classifier')\n",
        "    return model\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = 'accuracy')\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          epochs = 200,\n",
        "          batch_size = 5)\n",
        "\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "print(y_pred[0], y_test[0])\n",
        "y_pred = y_pred.flatten()\n",
        "y_test.shape,y_pred.shape \n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# 함수 인풋 배열은 정수가 되야 된다.\n",
        "cm = confusion_matrix(y_test, np.round(y_pred))\n",
        "print(cm)\n",
        "ConfusionMatrixDisplay(cm).plot(cmap='Blues')\n",
        "\n"
      ],
      "metadata": {
        "id": "v5WYSd9a1Cz4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b08c2ba-7e27-4dec-feef-319c2787ce4d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(145, 60) (63, 60) (145,) (63,)\n",
            "[-0.73091423  0.14791278  0.43217734 -0.193733   -0.27973353 -1.17141213\n",
            " -1.11368502 -1.1830759  -0.61643936  0.02789771 -0.54396541 -0.43025161\n",
            "  0.20832115 -0.57696915 -1.27359777 -0.60100177 -0.1911599  -0.2737382\n",
            "  0.34226731  0.25514539 -0.11259861 -0.65372658 -0.80968198 -1.27254722\n",
            " -0.12615184  0.8203173   0.79833763 -0.82844031 -1.28832161 -1.57084217\n",
            " -1.63565539 -0.34053744  0.08145143 -0.41196074 -0.34881305  0.09887612\n",
            "  0.77837874  0.34473565  1.128427    1.06935769  0.14029395  0.01132786\n",
            " -0.08251782  0.11825963 -0.54032991 -0.66033662 -0.69576299 -0.9575622\n",
            " -0.19317929 -0.27318109 -0.69023617 -0.53275402 -0.71118685  0.88684983\n",
            "  1.34495706 -0.49309408  0.62024838  1.80515572  1.09602656 -0.0612105 ] 1\n",
            "Epoch 1/200\n",
            "29/29 [==============================] - 1s 3ms/step - loss: 5.3745e-06 - accuracy: 1.0000\n",
            "Epoch 2/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.8229e-06 - accuracy: 1.0000\n",
            "Epoch 3/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 3.2729e-06 - accuracy: 1.0000\n",
            "Epoch 4/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.6620e-06 - accuracy: 1.0000\n",
            "Epoch 5/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 2.1983e-06 - accuracy: 1.0000\n",
            "Epoch 6/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.8536e-06 - accuracy: 1.0000\n",
            "Epoch 7/200\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.6941e-06 - accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 1.4109e-06 - accuracy: 1.0000\n",
            "Epoch 9/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.2439e-06 - accuracy: 1.0000\n",
            "Epoch 10/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.1067e-06 - accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.0162e-06 - accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 9.2256e-07 - accuracy: 1.0000\n",
            "Epoch 13/200\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 8.3955e-07 - accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 7.1733e-07 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 6.5842e-07 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "29/29 [==============================] - 0s 4ms/step - loss: 6.1239e-07 - accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 5.5379e-07 - accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 5.1730e-07 - accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 4.8436e-07 - accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.4445e-07 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.2032e-07 - accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.8116e-07 - accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.5832e-07 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.3931e-07 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.1584e-07 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.0024e-07 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.8445e-07 - accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.6752e-07 - accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.5468e-07 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.4334e-07 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.3143e-07 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.1791e-07 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.1074e-07 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.9682e-07 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.8996e-07 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.7969e-07 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.7247e-07 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.6558e-07 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.5800e-07 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.5238e-07 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.4589e-07 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.4020e-07 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.3495e-07 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2931e-07 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2502e-07 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2044e-07 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1616e-07 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1264e-07 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.0781e-07 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.0482e-07 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.0051e-07 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 9.7736e-08 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 9.4752e-08 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 9.1635e-08 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 8.9117e-08 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 8.5909e-08 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 8.3422e-08 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 8.0767e-08 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 7.8467e-08 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 7.6254e-08 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 7.4275e-08 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 7.1569e-08 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 7.0037e-08 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 6.7821e-08 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 6.6108e-08 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 6.4509e-08 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 6.3136e-08 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 6.1008e-08 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 5.9309e-08 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 5.7856e-08 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 5.6410e-08 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 5.4931e-08 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 5.3437e-08 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 5.1816e-08 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 5.1299e-08 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.9422e-08 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.8472e-08 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.7327e-08 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.6073e-08 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.5149e-08 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.4059e-08 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.2920e-08 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.1912e-08 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.1353e-08 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 4.0177e-08 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.9220e-08 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.8532e-08 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.7476e-08 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.6831e-08 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.6009e-08 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.4974e-08 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.4375e-08 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 3.3674e-08 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.2994e-08 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.2439e-08 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.1736e-08 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.1042e-08 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 3.0475e-08 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.9856e-08 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.9393e-08 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.8732e-08 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.8364e-08 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.7817e-08 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.7215e-08 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.6762e-08 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.6431e-08 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.5942e-08 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.5480e-08 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.4996e-08 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.4677e-08 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.4274e-08 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.3943e-08 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.3424e-08 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.3070e-08 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.2698e-08 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.2373e-08 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.2030e-08 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.1766e-08 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.1381e-08 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.1039e-08 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.0794e-08 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.0558e-08 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 2.0179e-08 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.9950e-08 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.9662e-08 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.9461e-08 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.9142e-08 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.8974e-08 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.8729e-08 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.8486e-08 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.8282e-08 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.8073e-08 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.7872e-08 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.7620e-08 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.7456e-08 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.7207e-08 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.6983e-08 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.6808e-08 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.6645e-08 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.6548e-08 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.6283e-08 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.6151e-08 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.5966e-08 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.5848e-08 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.5594e-08 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.5450e-08 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.5333e-08 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.5169e-08 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.5068e-08 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.4907e-08 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.4687e-08 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.4530e-08 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.4409e-08 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.4278e-08 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.4085e-08 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.3970e-08 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.3875e-08 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.3779e-08 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.3637e-08 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.3540e-08 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.3440e-08 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.3315e-08 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.3150e-08 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.3016e-08 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2943e-08 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2863e-08 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2785e-08 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2724e-08 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2619e-08 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2515e-08 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2476e-08 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2400e-08 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2282e-08 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2262e-08 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2183e-08 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2055e-08 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1999e-08 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.2005e-08 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1831e-08 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1810e-08 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1732e-08 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1704e-08 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1668e-08 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1577e-08 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1487e-08 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1493e-08 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1452e-08 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1381e-08 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1359e-08 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1210e-08 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1153e-08 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.1157e-08 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1036e-08 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1073e-08 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.1044e-08 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.0980e-08 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.0940e-08 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.0859e-08 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "29/29 [==============================] - 0s 3ms/step - loss: 1.0891e-08 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 1.0829e-08 - accuracy: 1.0000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.2440 - accuracy: 0.8889\n",
            "[0.83374697] 1\n",
            "[[29  5]\n",
            " [ 2 27]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f3728ca4cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | 과제 3. 피마 인디언 당뇨병 예측\n",
        "\n",
        "\n",
        "* `pima-indians-diabetes.csv`\n",
        "```\n",
        "df = pd.read_csv(data_path,\n",
        "                names = [\"pregnant\", \"plasma\", \"pressure\", \"thickness\", \"insulin\", \"BMI\", \"pedigree\", \"age\", \"class\"])\n",
        "```"
      ],
      "metadata": {
        "id": "2K0mAdPzjOAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/SkillTreePython-DeepLearning/dataset/pima-indians-diabetes.csv'\n",
        "\n",
        "df = pd.read_csv(data_path,\n",
        "              names = [\"pregnant\", \"plasma\", \"pressure\", \"thickness\", \"insulin\", \"BMI\", \"pedigree\", \"age\", \"class\"])\n",
        "print(df.shape)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "F4voyB5gK3dp",
        "outputId": "e9ee3e1c-3490-424e-c4d3-32579925cf63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768, 9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   pregnant  plasma  pressure  thickness  insulin   BMI  pedigree  age  class\n",
              "0         6     148        72         35        0  33.6     0.627   50      1\n",
              "1         1      85        66         29        0  26.6     0.351   31      0\n",
              "2         8     183        64          0        0  23.3     0.672   32      1\n",
              "3         1      89        66         23       94  28.1     0.167   21      0\n",
              "4         0     137        40         35      168  43.1     2.288   33      1\n",
              "5         5     116        74          0        0  25.6     0.201   30      0\n",
              "6         3      78        50         32       88  31.0     0.248   26      1\n",
              "7        10     115         0          0        0  35.3     0.134   29      0\n",
              "8         2     197        70         45      543  30.5     0.158   53      1\n",
              "9         8     125        96          0        0   0.0     0.232   54      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0074a9bb-69da-4be7-9b37-cd03c6dc7479\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pregnant</th>\n",
              "      <th>plasma</th>\n",
              "      <th>pressure</th>\n",
              "      <th>thickness</th>\n",
              "      <th>insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>pedigree</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>116</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.6</td>\n",
              "      <td>0.201</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>78</td>\n",
              "      <td>50</td>\n",
              "      <td>32</td>\n",
              "      <td>88</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.248</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10</td>\n",
              "      <td>115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35.3</td>\n",
              "      <td>0.134</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>197</td>\n",
              "      <td>70</td>\n",
              "      <td>45</td>\n",
              "      <td>543</td>\n",
              "      <td>30.5</td>\n",
              "      <td>0.158</td>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>8</td>\n",
              "      <td>125</td>\n",
              "      <td>96</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.232</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0074a9bb-69da-4be7-9b37-cd03c6dc7479')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0074a9bb-69da-4be7-9b37-cd03c6dc7479 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0074a9bb-69da-4be7-9b37-cd03c6dc7479');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import models, layers, utils\n",
        "\n",
        "data_path = '/content/SkillTreePython-DeepLearning/dataset/pima-indians-diabetes.csv'\n",
        "\n",
        "def path2data_pima(path, seed=1):\n",
        "    # 데이터 적절히 불러오기\n",
        "    df = pd.read_csv(data_path,\n",
        "                     header=None, # 첫번째 행이 데이터 (칼럼이 없음)\n",
        "                     )\n",
        "    # x-y 분할\n",
        "    x = df.values[:,0:-1] # 모든 행(샘플), 0부터 59까지 열(속성)\n",
        "    y = df.values[:,-1] # 모든 행(샘플), 60번째 열(속성)\n",
        "    \n",
        "    # 정규화 (x의 모든 속성, y는 하지 않음)\n",
        "    scaler = StandardScaler()\n",
        "    x = scaler.fit_transform(x)\n",
        "\n",
        "    # 라벨링 (y 라벨링, x는 하지 않음)\n",
        "    labeling = LabelEncoder()\n",
        "    y = labeling.fit_transform(y)\n",
        "\n",
        "    # train-test 데이터 분할\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3,\n",
        "                                                        random_state=seed, stratify=y) # stratify는 train_test 분할 시 클레스 비율을 비슷하게 유지해 줍니다.\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# data_path = '/content/SkillTreePython-DeepLearning/dataset/sonar.csv'\n",
        "x_train, x_test, y_train, y_test = path2data_pima(data_path)\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "print(x_train[1], y_train[1])\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    x = layers.Input(shape=(8,)) # 데이터가 주어지면 인풋의 shape는 고정\n",
        "    z = layers.Dense(5,activation='relu')(x)\n",
        "    z = layers.Dense(2,activation='relu')(z)\n",
        "    y = layers.Dense(1, activation='sigmoid')(z) # 데이터가 주어지면 아웃풋의 shape는 고정 \n",
        "    # (이진분류문제는 마지막 아웃풋 shape가 1, 마지막 활성함수는 sigmoid를 사용합니다.)\n",
        "    model = models.Model(x,y, name='pima_classifier')\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "# 시각화\n",
        "utils.plot_model(model, show_shapes=True)\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = 'accuracy')\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          epochs = 350,\n",
        "          batch_size = 3)\n",
        "\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "print(y_pred[0], y_test[0])\n",
        "y_pred = y_pred.flatten()\n",
        "y_test.shape,y_pred.shape \n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# 함수 인풋 배열은 정수가 되야 된다.\n",
        "cm = confusion_matrix(y_test, np.round(y_pred))\n",
        "print(cm)\n",
        "ConfusionMatrixDisplay(cm).plot(cmap='Blues')"
      ],
      "metadata": {
        "id": "Ppi8p2S_HsVb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9b8b9c4-033f-4635-c784-91c81b381db0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(537, 8) (231, 8) (537,) (231,)\n",
            "[-0.84488505 -0.74783062  0.04624525  1.22091023 -0.69289057  0.77514938\n",
            " -0.76673656 -0.27575966] 0\n",
            "Model: \"pima_classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 8)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 45        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 12        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60\n",
            "Trainable params: 60\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/350\n",
            "179/179 [==============================] - 1s 2ms/step - loss: 0.6843 - accuracy: 0.5698\n",
            "Epoch 2/350\n",
            "179/179 [==============================] - 0s 1ms/step - loss: 0.6498 - accuracy: 0.7337\n",
            "Epoch 3/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.6266 - accuracy: 0.7635\n",
            "Epoch 4/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.6064 - accuracy: 0.7747\n",
            "Epoch 5/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5886 - accuracy: 0.7821\n",
            "Epoch 6/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7858\n",
            "Epoch 7/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5586 - accuracy: 0.7840\n",
            "Epoch 8/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5471 - accuracy: 0.7877\n",
            "Epoch 9/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7877\n",
            "Epoch 10/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.7858\n",
            "Epoch 11/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.7914\n",
            "Epoch 12/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7896\n",
            "Epoch 13/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7952\n",
            "Epoch 14/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7933\n",
            "Epoch 15/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.7952\n",
            "Epoch 16/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4927 - accuracy: 0.7877\n",
            "Epoch 17/350\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 0.4889 - accuracy: 0.7989\n",
            "Epoch 18/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4858 - accuracy: 0.7933\n",
            "Epoch 19/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4827 - accuracy: 0.7933\n",
            "Epoch 20/350\n",
            "179/179 [==============================] - 1s 4ms/step - loss: 0.4803 - accuracy: 0.7970\n",
            "Epoch 21/350\n",
            "179/179 [==============================] - 1s 4ms/step - loss: 0.4785 - accuracy: 0.7933\n",
            "Epoch 22/350\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 0.4760 - accuracy: 0.7952\n",
            "Epoch 23/350\n",
            "179/179 [==============================] - 1s 4ms/step - loss: 0.4749 - accuracy: 0.7933\n",
            "Epoch 24/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4721 - accuracy: 0.7933\n",
            "Epoch 25/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4704 - accuracy: 0.7970\n",
            "Epoch 26/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4700 - accuracy: 0.7952\n",
            "Epoch 27/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.7952\n",
            "Epoch 28/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.7952\n",
            "Epoch 29/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4657 - accuracy: 0.7989\n",
            "Epoch 30/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4649 - accuracy: 0.7970\n",
            "Epoch 31/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4635 - accuracy: 0.7914\n",
            "Epoch 32/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4629 - accuracy: 0.7989\n",
            "Epoch 33/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7952\n",
            "Epoch 34/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4608 - accuracy: 0.7933\n",
            "Epoch 35/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.7933\n",
            "Epoch 36/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7914\n",
            "Epoch 37/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7952\n",
            "Epoch 38/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4574 - accuracy: 0.7970\n",
            "Epoch 39/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4568 - accuracy: 0.7914\n",
            "Epoch 40/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4554 - accuracy: 0.7933\n",
            "Epoch 41/350\n",
            "179/179 [==============================] - 0s 1ms/step - loss: 0.4549 - accuracy: 0.7933\n",
            "Epoch 42/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7933\n",
            "Epoch 43/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4541 - accuracy: 0.7933\n",
            "Epoch 44/350\n",
            "179/179 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.7933\n",
            "Epoch 45/350\n",
            "179/179 [==============================] - 0s 1ms/step - loss: 0.4523 - accuracy: 0.7914\n",
            "Epoch 46/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.7933\n",
            "Epoch 47/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.7914\n",
            "Epoch 48/350\n",
            "179/179 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.7933\n",
            "Epoch 49/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4497 - accuracy: 0.7952\n",
            "Epoch 50/350\n",
            "179/179 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.7933\n",
            "Epoch 51/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.7989\n",
            "Epoch 52/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.7970\n",
            "Epoch 53/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4471 - accuracy: 0.7970\n",
            "Epoch 54/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.7952\n",
            "Epoch 55/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.7989\n",
            "Epoch 56/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.7989\n",
            "Epoch 57/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.8026\n",
            "Epoch 58/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.8007\n",
            "Epoch 59/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.8026\n",
            "Epoch 60/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4416 - accuracy: 0.8026\n",
            "Epoch 61/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.8045\n",
            "Epoch 62/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.7970\n",
            "Epoch 63/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4398 - accuracy: 0.8026\n",
            "Epoch 64/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.7989\n",
            "Epoch 65/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8007\n",
            "Epoch 66/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.8026\n",
            "Epoch 67/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8045\n",
            "Epoch 68/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8026\n",
            "Epoch 69/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4365 - accuracy: 0.8026\n",
            "Epoch 70/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8045\n",
            "Epoch 71/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.8063\n",
            "Epoch 72/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8007\n",
            "Epoch 73/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8063\n",
            "Epoch 74/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4331 - accuracy: 0.8045\n",
            "Epoch 75/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.8101\n",
            "Epoch 76/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8007\n",
            "Epoch 77/350\n",
            "179/179 [==============================] - 0s 1ms/step - loss: 0.4306 - accuracy: 0.8082\n",
            "Epoch 78/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8101\n",
            "Epoch 79/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8045\n",
            "Epoch 80/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8063\n",
            "Epoch 81/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8119\n",
            "Epoch 82/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.8082\n",
            "Epoch 83/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8101\n",
            "Epoch 84/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.8101\n",
            "Epoch 85/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8101\n",
            "Epoch 86/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8045\n",
            "Epoch 87/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8082\n",
            "Epoch 88/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4275 - accuracy: 0.8119\n",
            "Epoch 89/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.8082\n",
            "Epoch 90/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4266 - accuracy: 0.8026\n",
            "Epoch 91/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8101\n",
            "Epoch 92/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.8101\n",
            "Epoch 93/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.8063\n",
            "Epoch 94/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4252 - accuracy: 0.8082\n",
            "Epoch 95/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.8045\n",
            "Epoch 96/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8082\n",
            "Epoch 97/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4249 - accuracy: 0.8007\n",
            "Epoch 98/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4243 - accuracy: 0.8045\n",
            "Epoch 99/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.8007\n",
            "Epoch 100/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.8026\n",
            "Epoch 101/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8063\n",
            "Epoch 102/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4230 - accuracy: 0.8007\n",
            "Epoch 103/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4243 - accuracy: 0.8026\n",
            "Epoch 104/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.8026\n",
            "Epoch 105/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4218 - accuracy: 0.8063\n",
            "Epoch 106/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4230 - accuracy: 0.8045\n",
            "Epoch 107/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4225 - accuracy: 0.8007\n",
            "Epoch 108/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4218 - accuracy: 0.8007\n",
            "Epoch 109/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4215 - accuracy: 0.8026\n",
            "Epoch 110/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4217 - accuracy: 0.7989\n",
            "Epoch 111/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4210 - accuracy: 0.7989\n",
            "Epoch 112/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4208 - accuracy: 0.8045\n",
            "Epoch 113/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4211 - accuracy: 0.8026\n",
            "Epoch 114/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4199 - accuracy: 0.8082\n",
            "Epoch 115/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8045\n",
            "Epoch 116/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4198 - accuracy: 0.8026\n",
            "Epoch 117/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4198 - accuracy: 0.8082\n",
            "Epoch 118/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.7989\n",
            "Epoch 119/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.8026\n",
            "Epoch 120/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4187 - accuracy: 0.8045\n",
            "Epoch 121/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4185 - accuracy: 0.8063\n",
            "Epoch 122/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4182 - accuracy: 0.8007\n",
            "Epoch 123/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8026\n",
            "Epoch 124/350\n",
            "179/179 [==============================] - 0s 1ms/step - loss: 0.4183 - accuracy: 0.8026\n",
            "Epoch 125/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4180 - accuracy: 0.8007\n",
            "Epoch 126/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8045\n",
            "Epoch 127/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.8045\n",
            "Epoch 128/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.8007\n",
            "Epoch 129/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.8045\n",
            "Epoch 130/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.8045\n",
            "Epoch 131/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4171 - accuracy: 0.8045\n",
            "Epoch 132/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4166 - accuracy: 0.8045\n",
            "Epoch 133/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8045\n",
            "Epoch 134/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4169 - accuracy: 0.8082\n",
            "Epoch 135/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8119\n",
            "Epoch 136/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8119\n",
            "Epoch 137/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8138\n",
            "Epoch 138/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4154 - accuracy: 0.8119\n",
            "Epoch 139/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4159 - accuracy: 0.8101\n",
            "Epoch 140/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4146 - accuracy: 0.8082\n",
            "Epoch 141/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.8101\n",
            "Epoch 142/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8045\n",
            "Epoch 143/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4139 - accuracy: 0.8063\n",
            "Epoch 144/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4142 - accuracy: 0.8119\n",
            "Epoch 145/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8082\n",
            "Epoch 146/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4141 - accuracy: 0.8082\n",
            "Epoch 147/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8082\n",
            "Epoch 148/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8082\n",
            "Epoch 149/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4130 - accuracy: 0.8101\n",
            "Epoch 150/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4123 - accuracy: 0.8138\n",
            "Epoch 151/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4129 - accuracy: 0.8119\n",
            "Epoch 152/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8138\n",
            "Epoch 153/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.8119\n",
            "Epoch 154/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.8101\n",
            "Epoch 155/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4129 - accuracy: 0.8082\n",
            "Epoch 156/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.8082\n",
            "Epoch 157/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4116 - accuracy: 0.8119\n",
            "Epoch 158/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.8082\n",
            "Epoch 159/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4112 - accuracy: 0.8119\n",
            "Epoch 160/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8101\n",
            "Epoch 161/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4112 - accuracy: 0.8156\n",
            "Epoch 162/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.8156\n",
            "Epoch 163/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.8101\n",
            "Epoch 164/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8138\n",
            "Epoch 165/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8101\n",
            "Epoch 166/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8119\n",
            "Epoch 167/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.8119\n",
            "Epoch 168/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8082\n",
            "Epoch 169/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.8101\n",
            "Epoch 170/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8138\n",
            "Epoch 171/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.8138\n",
            "Epoch 172/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4100 - accuracy: 0.8156\n",
            "Epoch 173/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8138\n",
            "Epoch 174/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4099 - accuracy: 0.8138\n",
            "Epoch 175/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8175\n",
            "Epoch 176/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4087 - accuracy: 0.8138\n",
            "Epoch 177/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8175\n",
            "Epoch 178/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8156\n",
            "Epoch 179/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8138\n",
            "Epoch 180/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4094 - accuracy: 0.8101\n",
            "Epoch 181/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8101\n",
            "Epoch 182/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8194\n",
            "Epoch 183/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8156\n",
            "Epoch 184/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8119\n",
            "Epoch 185/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8194\n",
            "Epoch 186/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.8156\n",
            "Epoch 187/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8156\n",
            "Epoch 188/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4083 - accuracy: 0.8156\n",
            "Epoch 189/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8101\n",
            "Epoch 190/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4076 - accuracy: 0.8101\n",
            "Epoch 191/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.8138\n",
            "Epoch 192/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8138\n",
            "Epoch 193/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8119\n",
            "Epoch 194/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4077 - accuracy: 0.8138\n",
            "Epoch 195/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8082\n",
            "Epoch 196/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8138\n",
            "Epoch 197/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.8156\n",
            "Epoch 198/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4067 - accuracy: 0.8138\n",
            "Epoch 199/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4069 - accuracy: 0.8119\n",
            "Epoch 200/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.8138\n",
            "Epoch 201/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8119\n",
            "Epoch 202/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8138\n",
            "Epoch 203/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8101\n",
            "Epoch 204/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4071 - accuracy: 0.8119\n",
            "Epoch 205/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8194\n",
            "Epoch 206/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4049 - accuracy: 0.8138\n",
            "Epoch 207/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.8119\n",
            "Epoch 208/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.4049 - accuracy: 0.8101\n",
            "Epoch 209/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4051 - accuracy: 0.8119\n",
            "Epoch 210/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.8175\n",
            "Epoch 211/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8156\n",
            "Epoch 212/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8156\n",
            "Epoch 213/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.8119\n",
            "Epoch 214/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8101\n",
            "Epoch 215/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4046 - accuracy: 0.8156\n",
            "Epoch 216/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.8101\n",
            "Epoch 217/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.8101\n",
            "Epoch 218/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.4049 - accuracy: 0.8138\n",
            "Epoch 219/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8101\n",
            "Epoch 220/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.8138\n",
            "Epoch 221/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8156\n",
            "Epoch 222/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8156\n",
            "Epoch 223/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8138\n",
            "Epoch 224/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4040 - accuracy: 0.8156\n",
            "Epoch 225/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8119\n",
            "Epoch 226/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8156\n",
            "Epoch 227/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8101\n",
            "Epoch 228/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8119\n",
            "Epoch 229/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.8194\n",
            "Epoch 230/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8138\n",
            "Epoch 231/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8119\n",
            "Epoch 232/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8194\n",
            "Epoch 233/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8175\n",
            "Epoch 234/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8194\n",
            "Epoch 235/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.8212\n",
            "Epoch 236/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8138\n",
            "Epoch 237/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.8156\n",
            "Epoch 238/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.8194\n",
            "Epoch 239/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8101\n",
            "Epoch 240/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8212\n",
            "Epoch 241/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8194\n",
            "Epoch 242/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8138\n",
            "Epoch 243/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8156\n",
            "Epoch 244/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8175\n",
            "Epoch 245/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8194\n",
            "Epoch 246/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4027 - accuracy: 0.8138\n",
            "Epoch 247/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8194\n",
            "Epoch 248/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8138\n",
            "Epoch 249/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8175\n",
            "Epoch 250/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8156\n",
            "Epoch 251/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8194\n",
            "Epoch 252/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8175\n",
            "Epoch 253/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.8156\n",
            "Epoch 254/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8194\n",
            "Epoch 255/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8175\n",
            "Epoch 256/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8175\n",
            "Epoch 257/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4020 - accuracy: 0.8175\n",
            "Epoch 258/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8156\n",
            "Epoch 259/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8156\n",
            "Epoch 260/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8175\n",
            "Epoch 261/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8156\n",
            "Epoch 262/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8175\n",
            "Epoch 263/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8101\n",
            "Epoch 264/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8212\n",
            "Epoch 265/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8175\n",
            "Epoch 266/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8175\n",
            "Epoch 267/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8138\n",
            "Epoch 268/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8156\n",
            "Epoch 269/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8156\n",
            "Epoch 270/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8175\n",
            "Epoch 271/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8194\n",
            "Epoch 272/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8194\n",
            "Epoch 273/350\n",
            "179/179 [==============================] - 1s 5ms/step - loss: 0.4001 - accuracy: 0.8119\n",
            "Epoch 274/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8194\n",
            "Epoch 275/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4002 - accuracy: 0.8194\n",
            "Epoch 276/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8250\n",
            "Epoch 277/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4000 - accuracy: 0.8156\n",
            "Epoch 278/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8194\n",
            "Epoch 279/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.8212\n",
            "Epoch 280/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8082\n",
            "Epoch 281/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8138\n",
            "Epoch 282/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8156\n",
            "Epoch 283/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8212\n",
            "Epoch 284/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8119\n",
            "Epoch 285/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8101\n",
            "Epoch 286/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3998 - accuracy: 0.8156\n",
            "Epoch 287/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4002 - accuracy: 0.8156\n",
            "Epoch 288/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.8156\n",
            "Epoch 289/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.4000 - accuracy: 0.8156\n",
            "Epoch 290/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8175\n",
            "Epoch 291/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.8156\n",
            "Epoch 292/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.8212\n",
            "Epoch 293/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8175\n",
            "Epoch 294/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8250\n",
            "Epoch 295/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.8156\n",
            "Epoch 296/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8231\n",
            "Epoch 297/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8175\n",
            "Epoch 298/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3998 - accuracy: 0.8231\n",
            "Epoch 299/350\n",
            "179/179 [==============================] - 1s 4ms/step - loss: 0.3993 - accuracy: 0.8175\n",
            "Epoch 300/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3988 - accuracy: 0.8175\n",
            "Epoch 301/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3997 - accuracy: 0.8175\n",
            "Epoch 302/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3993 - accuracy: 0.8212\n",
            "Epoch 303/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8138\n",
            "Epoch 304/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3980 - accuracy: 0.8194\n",
            "Epoch 305/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.3985 - accuracy: 0.8101\n",
            "Epoch 306/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3981 - accuracy: 0.8194\n",
            "Epoch 307/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8138\n",
            "Epoch 308/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3980 - accuracy: 0.8212\n",
            "Epoch 309/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3974 - accuracy: 0.8231\n",
            "Epoch 310/350\n",
            "179/179 [==============================] - 0s 3ms/step - loss: 0.3979 - accuracy: 0.8231\n",
            "Epoch 311/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3986 - accuracy: 0.8194\n",
            "Epoch 312/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3976 - accuracy: 0.8231\n",
            "Epoch 313/350\n",
            "179/179 [==============================] - 1s 3ms/step - loss: 0.3976 - accuracy: 0.8231\n",
            "Epoch 314/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8305\n",
            "Epoch 315/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8175\n",
            "Epoch 316/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8194\n",
            "Epoch 317/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.8231\n",
            "Epoch 318/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8212\n",
            "Epoch 319/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8212\n",
            "Epoch 320/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8231\n",
            "Epoch 321/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8231\n",
            "Epoch 322/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8175\n",
            "Epoch 323/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.8212\n",
            "Epoch 324/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8212\n",
            "Epoch 325/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8194\n",
            "Epoch 326/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.8175\n",
            "Epoch 327/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8287\n",
            "Epoch 328/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8156\n",
            "Epoch 329/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8268\n",
            "Epoch 330/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8212\n",
            "Epoch 331/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8175\n",
            "Epoch 332/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8156\n",
            "Epoch 333/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3968 - accuracy: 0.8231\n",
            "Epoch 334/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8212\n",
            "Epoch 335/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8212\n",
            "Epoch 336/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8268\n",
            "Epoch 337/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8175\n",
            "Epoch 338/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8156\n",
            "Epoch 339/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8194\n",
            "Epoch 340/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8268\n",
            "Epoch 341/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8175\n",
            "Epoch 342/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8175\n",
            "Epoch 343/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8175\n",
            "Epoch 344/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8231\n",
            "Epoch 345/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.8231\n",
            "Epoch 346/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8250\n",
            "Epoch 347/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8212\n",
            "Epoch 348/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8231\n",
            "Epoch 349/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8250\n",
            "Epoch 350/350\n",
            "179/179 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8175\n",
            "8/8 [==============================] - 0s 2ms/step - loss: 0.5853 - accuracy: 0.7273\n",
            "[0.0995295] 1\n",
            "[[129  21]\n",
            " [ 42  39]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fdce1216a90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEGCAYAAADxD4m3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYTklEQVR4nO3deZwV5Z3v8c+3u8UloqgoKmBgFBXEBYfBhcQhmlHMeKPOmAiaG8zoZUxwxbyi3snIRE2MmSQm4zKGqKNeDYoa4xo1MnoxriwuQZBIxAWDAoJGlAjd/Zs/TjVpkW6qTp/T51TxfedVr5yqU131A+TL81TV85QiAjOzImqodQFmZtXigDOzwnLAmVlhOeDMrLAccGZWWE21LqA9NW0e6tGz1mVYBsMG71LrEiyD1157lWXLlqkrx2jc6tMRzatS7Rurlj4YEaO7cr6uqK+A69GTTff4cq3LsAwef/qKWpdgGYw8YHiXjxHNq1L/Pf3zc1f27vIJu6CuAs7M8kCgfFzdcsCZWTYCGhprXUUqDjgzy05duozXbRxwZpaRu6hmVmRuwZlZIQm34MysqOQWnJkVmO+imlkx+SaDmRWVcBfVzArMLTgzKyZ3Uc2sqAQ05uMmQz5i2Mzqi5Ru2eBhdJ2kJZLmtNv275JekvSCpDsl9Wr33fmSFkiaL+mIDR3fAWdmGSVd1DTLhl0PrDtf3G+AoRGxD/B74HwASUOAMcBeyc9cJanTpqQDzsyyq1ALLiKmA8vX2fZQRDQnq08B/ZLPRwO3RMRHEbEQWACM6Oz4Djgzyy59C663pJntlvEZz/RPwK+Tz32BN9p9tyjZ1iHfZDCzbFK2zhLLIqKsaYQl/QvQDNxczs+DA87MylHloVqSTgKOAg6LiEg2vwn0b7dbv2Rbh9xFNbOMKnqT4ZNHl0YD3wK+GBEftvvqbmCMpE0lDQQGAc90diy34MwsuwoN1ZI0BRhF6VrdImASpbummwK/Uek8T0XEqRHxoqSpwFxKXdcJEdHS2fEdcGaWTQXng4uIsevZfG0n+38X+G7a4zvgzCwjD9UysyLzfHBmVlieLsnMCknuoppZkbkFZ2ZFJQecmRVRacZyB5yZFZGEGhxwZlZQbsGZWWE54MyssBxwZlZMSpYccMCZWSZCbsGZWXE1NHgkg5kVlFtwZlZMvgZnZkXmFpyZFZJvMphZoXmolpkVk9xFNbMCc8CZWWE54MyskHyTwcyKLR/55oAzs4zkoVpmVmDuoppZceUj3xxwXXX5v57IEZ8ZyrIV73PwmO8BcOEZx3DEZ4eyZk0LCxctY8KFN/GnlavYpKmRy/7vWIYN3oXW1lbO+9EdPD775Rr/CjZui95awdf/7UaWLn8fAeOOHcmpYz/Hrx6ezaWT72f+q28z7fpvMmzIp2tdal3JSwuuqh1pSaMlzZe0QNJ51TxXrUy59ymOO+PKj2175OmXOHjM9/jMCZfwh9eXMPGkw4HSXx6AkWO/x7GnXcHFZx2bm/9QiqqpqYGLz/oHnpr6bR76r29yze3TeemVxQzedWdu/MH/4eBhu9a6xLojKfVSa1ULOEmNwJXAkcAQYKykIdU6X6088ewfWPGnDz+27ZGnX6KlpRWAGXMWsnOfXgDsMXBHHpsxH4BlK1by3spVDBu8S/cWbB+zY++t2XfP/gD0/NRm7D5gRxYvfZc9Bu7IoAF9alxd/droAw4YASyIiFciYjVwC3B0Fc9Xl77yxYN4+Im5AMx5+U1GH7I3jY0N7LLzduy3Z3/69tmmxhVam9f/+A4vzF/EX+81oNal1D01KNVSa9W8BtcXeKPd+iLggHV3kjQeGA/AJltWsZzud87XjqC5uZWpv54BwE13P8nuA/rwyI3f4o3Fy3nmhYW0tLbWuEoDWPnhR3z13Gu4ZOI/stWWm9e6nLpXD62zNGp+kyEiJgOTARq22CFqXE7FjD3qAA7/zFCO+cZ/rN3W0tLKv1z2y7XrD147kT+8vqQW5Vk7a5pbGHfuz/nS6OH8r0P3q3U59a+Cg+0lXQccBSyJiKHJtm2BW4EBwKvAlyNihUon/SnwBeBD4KSImN3Z8avZRX0T6N9uvV+yrfAOO2gwZ/zvz3PCOT9j1Udr1m7ffNNN2GKzHgCMGrEnzc2tzF/4Vq3KNCAiOP2im9l9wI5MOPGwWpeTCwKkdEsK1wOj19l2HjAtIgYB05J1KF3PH5Qs44H/3NDBq9mCmwEMkjSQUrCNAU6o4vlq4pqLT2LkXw9iu15bMufei/j+5Ps5+6TD2bRHE3deeRoAM3/3KhO/fwu9t+3JHZdPoLU1WLz0XU6ddEONq7ennn+FW+9/hiG77cxnT7gEgH+d8EVWr27m3B/exrIVKzn+7KvZe/e+3HH5aTWutl5U7gZCREyXNGCdzUcDo5LPNwCPAucm22+MiACektRL0k4Rsbij41ct4CKiWdJpwINAI3BdRLxYrfPVyinfvv4T2266+8n17vvG4uWMOO6iKldkWRy0366smHHFer876nP7dnM1+dGQ/gZCb0kz261PTi5LdaZPu9B6C2i7nb2+6/p9ge4POICIuB+4v5rnMLNulr77CbAsIoaXe6qICEllX5uv+U0GM8sXkakFV46327qeknYC2u7EZb6un48pAcysrlTwJsP63A2MSz6PA+5qt/2rKjkQeK+z62/gFpyZlaGCj4lMoXRDobekRcAk4PvAVEknA68BX052v5/SIyILKD0m8rUNHd8BZ2bZdK119jERMbaDrz7xzE5y93RCluM74MwsEyFPeGlmxZWTkVoOODPLzmNRzayYKngNrtoccGaWSWksaj4SzgFnZpnlJN8ccGaWXZVHMlSMA87MsqngfHDV5oAzs0za5oPLAwecmWVUHy+UScMBZ2aZ5STfHHBmlpF8k8HMCsrPwZlZoTngzKywcpJvDjgzy84tODMrJg+2N7OiKk14mY+Ec8CZWWYNOWnCOeDMLLOc5JsDzsyykQfbm1mR5eQSXMcBJ+lyIDr6PiLOqEpFZlb3inCTYWa3VWFmuSFKd1LzoMOAi4gb2q9L2iIiPqx+SWZW73LSgGODb2+VdJCkucBLyfq+kq6qemVmVp9Umg8uzVJraV5P/RPgCOAdgIh4HjikmkWZWX2T0i21luouakS8sU4at1SnHDOrd6JYD/q+IelgICRtApwJzKtuWWZWz/JyFzVNF/VUYALQF/gjsF+ybmYbobTd03po5G2wBRcRy4ATu6EWM8uJvHRR09xF/StJ90haKmmJpLsk/VV3FGdm9Ukplw0eRzpb0ouS5kiaImkzSQMlPS1pgaRbJfUot840XdRfAFOBnYCdgduAKeWe0MzyrxKPiUjqC5wBDI+IoUAjMAa4FLgsInYDVgAnl1tnmoDbIiL+X0Q0J8tNwGblntDM8q10FzXdkkITsLmkJmALYDFwKHB78v0NwDHl1trZWNRtk4+/lnQecAulsanHA/eXe0IzyzllmvCyt6T2wz4nR8RkgIh4U9IPgdeBVcBDwCzg3YhoTvZfROkGZ1k6u8kwi1Kgtf1K/rnddwGcX+5JzSzfMoxSWBYRwzs4xjbA0cBA4F1Kl79GV6TARGdjUQdW8kRmVgxtXdQK+DywMCKWAkj6JTAS6CWpKWnF9QPeLPcEqUYySBoKDKHdtbeIuLHck5pZvlVonOnrwIGStqDURT2M0ixGjwDHUbosNg64q9wTbDDgJE0CRlEKuPuBI4HfAg44s41UJeItIp6WdDswG2gGngUmA/cBt0i6ONl2bbnnSNOCOw7YF3g2Ir4mqQ9wU7knNLN8k6CxQn3UiJgETFpn8yvAiEocP03ArYqIVknNkrYClgD9K3FyM8unepgKKY00ATdTUi/g55TurK4EnqxqVWZW13KSb6nGon4j+Xi1pAeArSLiheqWZWb1Sig3Y1E7e9B3/86+i4jZ1SnJzOpancwUkkZnLbgfdfJdUBpOUVGDd+vHlLsvqfRhrYoWLvmg1iVYBh81t1bkOLm/BhcRn+vOQswsHwQ05j3gzMw6kpMJfR1wZpadA87MCqk0HXk+Ei7NjL6S9BVJFyTru0iqyFPGZpZPFZwPrrp1ptjnKuAgYGyy/j5wZdUqMrO6V5iXzgAHRMT+kp4FiIgVXZkj3czyTUBTPaRXCmkCbo2kRkrPviFpe6AyD9OYWS7lJN9SBdx/AHcCO0j6LqXZRb5d1arMrG5JBRiq1SYibpY0i9JkdAKOiQi/2d5sI5aTfEs14eUuwIfAPe23RcTr1SzMzOpXPdwhTSNNF/U+/vLymc0ovSBiPrBXFesyszolKjfhZbWl6aLu3X49mWXkGx3sbmZFVyfPuKWReSRDRMyWdEA1ijGzfFBF3spQfWmuwU1st9oA7A/8sWoVmVldq+BrA6suTQuuZ7vPzZSuyd1RnXLMLA8KEXDJA749I+Kb3VSPmeVAXgbbdzZleVNENEsa2Z0FmVl9K702sNZVpNNZC+4ZStfbnpN0N3AbsHZ+6oj4ZZVrM7M6VZiRDJSefXuH0jsY2p6HC8ABZ7YRKspNhh2SO6hz+EuwtYmqVmVmdS0nDbhOA64R2BLW+8CLA85soyUaCvAc3OKIuLDbKjGzXBDFaMHl5JdgZt1K0JSTi3CdBdxh3VaFmeVGIVpwEbG8Owsxs/zIy2MiOXlcz8zqSaVeOiOpl6TbJb0kaZ6kgyRtK+k3kl5O/n+bcut0wJlZJqIUHGmWFH4KPBARewL7AvOA84BpETEImJasl8UBZ2bZqNRFTbN0ehhpa+AQ4FqAiFgdEe8CRwM3JLvdABxTbql+s72ZZVIayZD6GlxvSTPbrU+OiMnJ54HAUuC/JO0LzALOBPpExOJkn7eAPuXW6oAzs8wy3GJYFhHDO/iuidJ499Mj4mlJP2Wd7mhEhKSyBxa4i2pmmVXoJsMiYFFEPJ2s304p8N6WtFPpPNoJWFJunQ44M8tISOmWzkTEW8AbkvZINh0GzAXuBsYl28YBd5VbqbuoZpZJ213UCjkduFlSD+AV4GvJ4adKOhl4DfhyuQd3wJlZZpV60DcingPWd42uIiOpHHBmlo0KMGW5mdn6VLiLWlUOODPLzC04MyusfMSbA87MMhLQ6BacmRVVTvLNAWdmWQnlpJPqgDOzzNyCM7NCKj0mko+Ec8CZWTYpZ+utBw44M8ssL+9kcMCZWSalCS9rXUU6Djgzy8x3Uc2ssHLSQ3XAVUNLSysnTbyC7bfbih9fcBIX/OgW5i14k6bGRoYM6sf5E46lqamx1mUa8NHqNZxy7tWsXtNCS0sLh43cm69/5XCeeX4BP7n2PtY0tzB4t75ccOZxNDX6z6xNXlpwVZsUQNJ1kpZImlOtc9SrW+95nAH9d1i7fsTf7sfUqybyi8vP5KPVa7jroRk1rM7a67FJEz/73nhuveIsplx+Fk/O+j3Pz32VST+eyiXnnsBtV01kp+234d6HZ9W61LrRdg0uzVJr1Zz15HpgdBWPX5feXvYej8+cz9F/9zdrt40cvufaKZz32r0/S955r4YVWnuS2GLzTQFobm6huaWFhoYGNmlq5NN9twfggGGDmPbERvfvdMdSvjKwHu60Vi3gImI6sLxax69Xl11zL6eddCRazz9fzc0t/PqRZzlw/91rUJl1pKWllTGn/YTPn3gRB+w3iKF79Ke5pZW5Ly8CYNrjv+Ptpf5HqT2lXGqt5tfgJI0HxgPs1Ld/javpmt/OmMe2W3+Kwbv1ZdbvXvnE9z+4+i7222sAw/YaWIPqrCONjQ3ccsVZvL9yFedcfCN/eO1tLjn3BH7483tYs6aZA4ftTkM99LfqRMb3otZUzQMueQnsZIC99tm/7Pcf1oPn577G9Gfm8cSs+Xy0upkPPvyIST+6le+cczzXTHmYFe99wKXnn1jrMq0DPbfcnOH77MoTs+bz1X/8W677wdcBeHL273n9zaU1rq6+5CPe6iDgimTCuNFMGFe67Djrd69w853T+c45x3PXQzN46tmXueKiU2hoyMtkzxuHFe+tpKmxkZ5bbs6fP1rDU8+9zEnHjWL5uyvZtteWrF7TzPW3P8rJxx9a61LrS04SzgHXDS696lfsuEMvTvnWfwIw6qC9OGVMRV4aZF20dPn7TPrxVFpaW4kI/u4z+3DIiMFcdu19PPbMPCKC475wICP23a3WpdaVvHRRFVGdXqGkKcAooDfwNjApIq7t7Gf22mf/mHLf/69KPVYdmzS6RZonXzrys8x5fnaX0mnw3sPixrseTbXviF17zYqI9b0WsFtUrQUXEWOrdWwzq7F8NODcRTWzbEqPgOQj4RxwZpaN54MzsyLLSb454MwsK/nFz2ZWXDnJNwecmWVTL+NM03DAmVl2OUk4P6VpZpkp5f9SHUtqlPSspHuT9YGSnpa0QNKtknqUW6cDzswyk9ItKZ0JzGu3filwWUTsBqwATi63TgecmWWTMtzSBJykfsDfA9ck6wIOBW5PdrkBOKbcUn0Nzswyq+BIhp8A3wJ6JuvbAe9GRHOyvgjoW+7B3YIzs0xEphZcb0kz2y3j1x5HOgpYEhFVe+GFW3BmllmG9tuyTmYTGQl8UdIXgM2ArYCfAr0kNSWtuH7Am+XW6RacmWVXgZcyRMT5EdEvIgYAY4D/jogTgUeA45LdxgF3lVumA87MMqvyW7XOBSZKWkDpmlyn80h2xl1UM8us0s/5RsSjwKPJ51eAEZU4rgPOzLLLyUgGB5yZZeIJL82suDzhpZkVWU7yzQFnZll5wkszK7Cc5JsDzsyy8YSXZlZsOUk4B5yZZebHRMyssHwNzsyKSdDggDOz4spHwjngzCyTtgkv88ABZ2aZ5STfHHBmlp1bcGZWWB6qZWaFlY94c8CZWUYZX+pcUw44M8vMIxnMrLjykW8OODPLLif55oAzs6y69ErAbuWAM7NM8jSSwS9+NrPCcgvOzDLLSwvOAWdmmfkxETMrJj/oa2ZFlaebDA44M8vMXVQzKyy34MyssHKSbw44MytDThLOAWdmmQhyM1RLEVHrGtaStBR4rdZ1VEFvYFmti7BMivpn9umI2L4rB5D0AKXfnzSWRcTorpyvK+oq4IpK0syIGF7rOiw9/5kVg8eimllhOeDMrLAccN1jcq0LsMz8Z1YAvgZnZoXlFpyZFZYDzswKywFXRZJGS5ovaYGk82pdj22YpOskLZE0p9a1WNc54KpEUiNwJXAkMAQYK2lIbauyFK4HavZgqlWWA656RgALIuKViFgN3AIcXeOabAMiYjqwvNZ1WGU44KqnL/BGu/VFyTYz6yYOODMrLAdc9bwJ9G+33i/ZZmbdxAFXPTOAQZIGSuoBjAHurnFNZhsVB1yVREQzcBrwIDAPmBoRL9a2KtsQSVOAJ4E9JC2SdHKta7LyeaiWmRWWW3BmVlgOODMrLAecmRWWA87MCssBZ2aF5YDLEUktkp6TNEfSbZK26MKxrpd0XPL5ms4mApA0StLBZZzjVUmfePtSR9vX2WdlxnP9m6RvZq3Ris0Bly+rImK/iBgKrAZObf+lpLLecxsRp0TE3E52GQVkDjizWnPA5ddjwG5J6+oxSXcDcyU1Svp3STMkvSDpnwFUckUyP93DwA5tB5L0qKThyefRkmZLel7SNEkDKAXp2Unr8bOStpd0R3KOGZJGJj+7naSHJL0o6RpSvP9c0q8kzUp+Zvw6312WbJ8maftk266SHkh+5jFJe1biN9OKyW+2z6GkpXYk8ECyaX9gaEQsTELivYj4G0mbAo9LeggYBuxBaW66PsBc4Lp1jrs98HPgkORY20bEcklXAysj4ofJfr8ALouI30rahdJojcHAJOC3EXGhpL8H0owC+KfkHJsDMyTdERHvAJ8CZkbE2ZIuSI59GqWXwZwaES9LOgC4Cji0jN9G2wg44PJlc0nPJZ8fA66l1HV8JiIWJtsPB/Zpu74GbA0MAg4BpkREC/BHSf+9nuMfCExvO1ZEdDQv2ueBIdLaBtpWkrZMzvEPyc/eJ2lFil/TGZKOTT73T2p9B2gFbk223wT8MjnHwcBt7c69aYpz2EbKAZcvqyJiv/Ybkr/oH7TfBJweEQ+us98XKlhHA3BgRPx5PbWkJmkUpbA8KCI+lPQosFkHu0dy3nfX/T0w64ivwRXPg8DXJW0CIGl3SZ8CpgPHJ9fodgI+t56ffQo4RNLA5Ge3Tba/D/Rst99DwOltK5LaAmc6cEKy7Uhgmw3UujWwIgm3PSm1INs0AG2t0BModX3/BCyU9KXkHJK07wbOYRsxB1zxXEPp+trs5MUpP6PUUr8TeDn57kZKM2Z8TEQsBcZT6g4+z1+6iPcAx7bdZADOAIYnNzHm8pe7ud+hFJAvUuqqvr6BWh8AmiTNA75PKWDbfACMSH4NhwIXJttPBE5O6nsRTwNvnfBsImZWWG7BmVlhOeDMrLAccGZWWA44MyssB5yZFZYDzswKywFnZoX1P2cGx37atEkPAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### | 과제 4. 생각해보기\n",
        "\n",
        "데이터가 불균형 할 때 딥러닝에서는 어떠한 방법을 써서 이를 해결 하나요? \n"
      ],
      "metadata": {
        "id": "s4VjzrycjTUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "불균형 데이터 상태 그대로 예측하게 된다면 과적합 문제가 발생 할 수 있습니다. 대표적으로 과적합은 변수가 많아서 생기는 모델 복잡성 증가, 데이터 불균형으로 생기는 문제 등의 다양한 발생 원인들이 존재합니다.\n",
        "\n",
        "데이터 불균형성 해결 방법은 크게 1. Under Sampling 2. Over Sampling, 2가지가 있습니다.\n",
        "\n",
        "Under Sampling은 Down Sampling라고도 불리며 데이터의 분포가 높은 값을 낮은 값으로 맞춰주는 작업을 거치는 것을 말합니다.\n",
        "\n",
        "오버 샘플링은 Up Sampling라고도 불리며 분포가 작은 클래스의 값을 분포가 큰 클래스로 맞춰주는 샘플링 방법입니다."
      ],
      "metadata": {
        "id": "z4Af7h56SZur"
      }
    }
  ]
}